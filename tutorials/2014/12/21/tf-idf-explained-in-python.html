<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Basic Statistical NLP Part 1: Jaccard Similarity and TF-IDF</title>
        <meta name="viewport" content="width=device-width">
        <link rel="stylesheet" href="/css/normalize.css">
        <link rel="stylesheet" href="/css/skeleton.css">

        <!-- syntax highlighting CSS -->
        <link rel="stylesheet" href="/css/github.css">

        <!-- Custom CSS -->
        <link rel="stylesheet" href="/css/main.css">

    </head>
    <body>

        <div class="container">
              <div class="row">
   <div class="twelve columns personal-intro">
      <h2>The Portfolio & Writings of Bill Chambers</h2>
   </div>
</div>
<div class="row">
   <div class="one offset-by-three columns nav-blocks">
      <a onmouseover="vis_on('hhm');" onmouseout="vis_off('hhm');" href="/"><i class="mdi mdi-home"></i></a>
   </div>
   <div class="one columns nav-blocks">
      <a onmouseover="vis_on('hab');" onmouseout="vis_off('hab');" href="/about.html"><i class="mdi mdi-account"></i></a>
   </div>
   <div class="one columns nav-blocks">
      <a onmouseover="vis_on('hes');" onmouseout="vis_off('hes');" href="/essays.html"><i class="mdi mdi-file-document"></i></a>
   </div>
   <div class="one columns nav-blocks">
      <a onmouseover="vis_on('hpr');" onmouseout="vis_off('hpr');" href="/projects.html"><i class="mdi mdi-beaker"></i></a>
   </div>
   <div class="one columns nav-blocks">
      <a onmouseover="vis_on('htu');" onmouseout="vis_off('htu');" href="/tutorials.html"><i class="mdi mdi-cog"></i></a>
   </div>
   <div class="one columns nav-blocks">
      <a onmouseover="vis_on('hem');" onmouseout="vis_off('hem');" href="mailto:wchambers@ischool.berkeley.edu"><i class="mdi mdi-email"></i></a>
   </div>
</div>
<div class="row">
   <div class="one offset-by-three columns nav-blocks">
      <a onmouseover="vis_on('hhm');" onmouseout="vis_off('hhm');" id='hhm' class='sibling' href="/">home</a>
   </div>
   <div class="one columns nav-blocks">
      <a onmouseover="vis_on('hab');" onmouseout="vis_off('hab');" id='hab' class='sibling' href="/about.html">about</a>
   </div>
   <div class="one columns nav-blocks">
      <a onmouseover="vis_on('hes');" onmouseout="vis_off('hes');" id='hes' class='sibling' href="/essays.html">essays</a>
   </div>
   <div class="one columns nav-blocks">
      <a onmouseover="vis_on('hpr');" onmouseout="vis_off('hpr');" id='hpr' class='sibling' href="/projects.html">projects</a>
   </div>
   <div class="one columns nav-blocks">
      <a onmouseover="vis_on('htu');" onmouseout="vis_off('htu');" id='htu' class='sibling' href="/tutorials.html">tutorials</a>
   </div>
   <div class="one columns nav-blocks">
      <a onmouseover="vis_on('hem');" onmouseout="vis_off('hem');" id='hem' class='sibling' href="mailto:wchambers@ischool.berkeley.edu">email</a>
   </div>
</div>

              <div class="row">
   <div class="ten offset-by-one columns">
      <article class="post">
         <div class='titles'>
            <h4>Basic Statistical NLP Part 1: Jaccard Similarity and TF-IDF</h4>
            <p class="meta">
               <!-- <span class='tags'>
                  Tags:
                  
                  
                  </span> -->
               <span class='date'>
               21 December 2014
               </span>
            </p>
         </div>
         <p><em><a href="/tutorials/2014/12/22/cosine-similarity-explained-in-python.html">This is a two part post, you can see part 2 here.</a></em></p>

<p>In my natural language processing class we&#39;ve been playing around with similarity measures and I thought it would be useful to write them up here. Obviously this isn&#39;t an exhaustive list but I think it would be a good resource for anyone looking to learn a bit more about ways of measuring similarity between documents.</p>

<p>One of the first steps in many NLP operations is tokenization. Tokenization is the process by which we split a string into a list of &quot;tokens&quot; or words. While the below formula isn&#39;t exhaustive, it works well for simple things.</p>

<p>Here&#39;s the code in python: </p>
<div class="highlight"><pre><code class="py language-py" data-lang="py"><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="n">tokenize</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">doc</span><span class="p">:</span> <span class="n">doc</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&quot; &quot;</span><span class="p">)</span>
</code></pre></div>
<p>Now of course we&#39;ll need some documents.</p>
<div class="highlight"><pre><code class="py language-py" data-lang="py"><span class="n">document_0</span> <span class="o">=</span> <span class="s">&quot;China has a strong economy that is growing at a rapid pace. However politically it differs greatly from the US Economy.&quot;</span>
<span class="n">document_1</span> <span class="o">=</span> <span class="s">&quot;At last, China seems serious about confronting an endemic problem: domestic violence and corruption.&quot;</span>
<span class="n">document_2</span> <span class="o">=</span> <span class="s">&quot;Japan&#39;s prime minister, Shinzo Abe, is working towards healing the economic turmoil in his own country for his view on the future of his people.&quot;</span>
<span class="n">document_3</span> <span class="o">=</span> <span class="s">&quot;Vladimir Putin is working hard to fix the economy in Russia as the Ruble has tumbled.&quot;</span>
<span class="n">document_4</span> <span class="o">=</span> <span class="s">&quot;What&#39;s the future of Abenomics? We asked Shinzo Abe for his views&quot;</span>
<span class="n">document_5</span> <span class="o">=</span> <span class="s">&quot;Obama has eased sanctions on Cuba while accelerating those against the Russian Economy, even as the Ruble&#39;s value falls almost daily.&quot;</span>
<span class="n">document_6</span> <span class="o">=</span> <span class="s">&quot;Vladimir Putin was found to be riding a horse, again, without a shirt on while hunting deer. Vladimir Putin always seems so serious about things - even riding horses.&quot;</span>

<span class="n">all_documents</span> <span class="o">=</span> <span class="p">[</span><span class="n">document_0</span><span class="p">,</span> <span class="n">document_1</span><span class="p">,</span> <span class="n">document_2</span><span class="p">,</span> <span class="n">document_3</span><span class="p">,</span> <span class="n">document_4</span><span class="p">,</span> <span class="n">document_5</span><span class="p">,</span> <span class="n">document_6</span><span class="p">]</span>

<span class="n">tokenized_documents</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenize</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">all_documents</span><span class="p">]</span> <span class="c"># tokenized docs</span>
<span class="n">all_tokens_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="n">item</span> <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">tokenized_documents</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">])</span>
</code></pre></div>
<p>These are just a random set of documents that I&#39;ve written. Some are derived from some tweets that I read, others are just made up entirely. However we can see that there are some that are more similar to one another than others.</p>

<h3>Jaccard Similarity</h3>

<hr>

<p>Jaccard Similarity is the simplest of the similarities and is nothing more than a combination of binary operations of set algebra. To calculate the Jaccard Distance or similarity is treat our document as a set of tokens. Mathematically the formula is as follows:</p>

<p><img src="https://upload.wikimedia.org/math/1/8/6/186c7f4e83da32e889d606140fae25a0.png" alt="Jaccard Similarity"></p>

<p>source: Wikipedia</p>

<p>It&#39;s simply the length of the intersection of the sets of tokens divided by the length of the union of the two sets.</p>

<p>In Python we can write the Jaccard Similarity as follows:</p>
<div class="highlight"><pre><code class="py language-py" data-lang="py"><span class="k">def</span> <span class="nf">jaccard_similarity</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">document</span><span class="p">):</span>
    <span class="n">intersection</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">query</span><span class="p">)</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">document</span><span class="p">))</span>
    <span class="n">union</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">query</span><span class="p">)</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">document</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">intersection</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">union</span><span class="p">)</span>
</code></pre></div>
<p>Here we are running it on a couple of the documents:</p>
<div class="highlight"><pre><code class="py language-py" data-lang="py"><span class="c"># comparing document_2 and document_4</span>
<span class="n">jaccard_similarity</span><span class="p">(</span><span class="n">tokenized_documents</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span><span class="n">tokenized_documents</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
<span class="c"># 0.16666666666666666</span>
</code></pre></div>
<p>We can see that these documents are a better match, but still far from perfect. There are a couple of things throwing us off. Document length has a big effect and common words carry the same weight as non common words. Let&#39;s take a look at the intersection to understand what I&#39;m talking about.</p>
<div class="highlight"><pre><code class="py language-py" data-lang="py"><span class="nb">set</span><span class="p">(</span><span class="n">tokenized_documents</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">tokenized_documents</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
<span class="c"># {&#39;abe&#39;, &#39;his&#39;, &#39;shinzo&#39;, &#39;the&#39;}</span>
</code></pre></div>
<p>As we can see, &quot;his&quot; and &quot;the&quot; have the same weight as &quot;abe&quot; and &quot;shinzo&quot;. Let&#39;s take a more extreme example.</p>
<div class="highlight"><pre><code class="py language-py" data-lang="py"><span class="n">jaccard_similarity</span><span class="p">(</span><span class="n">tokenized_documents</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">tokenized_documents</span><span class="p">[</span><span class="mi">6</span><span class="p">])</span>
<span class="c"># 0.08571428571428572</span>
<span class="nb">set</span><span class="p">(</span><span class="n">tokenized_documents</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">tokenized_documents</span><span class="p">[</span><span class="mi">6</span><span class="p">]))</span>
<span class="c"># {&#39;about&#39;, &#39;seems&#39;, &#39;serious&#39;}</span>
</code></pre></div>
<p>These documents have nothing in common. However they are rated as 8% relevant. This examples identifies the fundamental issues with Jaccard Similarity:</p>

<ol>
<li>Length is irrelevant. (bias towards longer documents).</li>
<li>Words that appear in a lot of documents are weighted the same as those that appear in few. (bias towards longer documents as well as non-descriptive words)</li>
</ol>

<p>We need a way to weigh certain words differently than others. Words that appear in all the documents are not going to be good at identifying documents because of the fact that, well...they appear in all the documents. Let&#39;s move onto another similarity measure that takes this into account TF-IDF and Cosine Similarity.</p>

<p>References:</p>

<ul>
<li><a href="http://en.wikipedia.org/wiki/Jaccard_index">Jaccard Similarity on Wikipedia</a></li>
</ul>

<h3>TF-IDF</h3>

<hr>

<p>TF, or Term Frequency, measures one thing: the count of words in a document. It&#39;s the first step for TF-IDF or Term Frequency Inverse Document Frequency.</p>

<p>First comes the easy part.</p>
<div class="highlight"><pre><code class="py language-py" data-lang="py"><span class="k">def</span> <span class="nf">term_frequency</span><span class="p">(</span><span class="n">term</span><span class="p">,</span> <span class="n">tokenized_document</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenized_document</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">term</span><span class="p">)</span>
</code></pre></div>
<p>Above we&#39;ve created a simple way of counting the number of occurrences of a token in a document. There are plenty of ways to do this, through collections or for loops but they all end with the same result, how many times does a token appear in a document.</p>

<p>However this does not resolve the issue we&#39;ve mentioned above - this is still biased towards longer documents. To remedy this we should weight terms according to document length or normalize them on another scale. This process is called normalization. While there are several ways to do this, I&#39;ll only show two, sub-linear term frequency and augmented frequency measurement. Here&#39;s the formula:</p>

<p><img src="https://upload.wikimedia.org/math/5/c/c/5cc18acd4dbd9be636047fc2a7a10105.png" alt="augmented frequency"></p>
<div class="highlight"><pre><code class="py language-py" data-lang="py"><span class="k">def</span> <span class="nf">sublinear_term_frequency</span><span class="p">(</span><span class="n">term</span><span class="p">,</span> <span class="n">tokenized_document</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tokenized_document</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">term</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">augmented_term_frequency</span><span class="p">(</span><span class="n">term</span><span class="p">,</span> <span class="n">tokenized_document</span><span class="p">):</span>
    <span class="n">max_count</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="n">term_frequency</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">tokenized_document</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tokenized_document</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">+</span> <span class="p">((</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">term_frequency</span><span class="p">(</span><span class="n">term</span><span class="p">,</span> <span class="n">tokenized_document</span><span class="p">))</span><span class="o">/</span><span class="n">max_count</span><span class="p">))</span>
</code></pre></div>
<p>These two different normalization techniques obviously approach our goal differently. I&#39;ll be focusing on the sub-linear term frequency as it will tie into exploring Tfidf with <a href="http://scikit-learn.org/stable/">Scikit-Learn</a>.</p>

<p>The next part of TF-IDF is the <code>IDF</code> or inverse document frequency. Basically we want to target the words that are unique to certain documents instead of those that appear in all the documents because by definition, those are not good identifiers for any given document. Here&#39;s our equation for IDF.</p>

<p><img src="http://upload.wikimedia.org/math/b/a/e/bae842b33a4cafc0f22519cf960b052a.png" alt="IDF"></p>
<div class="highlight"><pre><code class="py language-py" data-lang="py"><span class="k">def</span> <span class="nf">inverse_document_frequencies</span><span class="p">(</span><span class="n">tokenized_documents</span><span class="p">):</span>
    <span class="n">idf_values</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">all_tokens_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="n">item</span> <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">tokenized_documents</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">tkn</span> <span class="ow">in</span> <span class="n">all_tokens_set</span><span class="p">:</span>
        <span class="n">contains_token</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">doc</span><span class="p">:</span> <span class="n">tkn</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">,</span> <span class="n">tokenized_documents</span><span class="p">)</span>
        <span class="n">idf_values</span><span class="p">[</span><span class="n">tkn</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenized_documents</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">contains_token</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">idf_values</span>
</code></pre></div>
<p>Now we&#39;ve got a weight for every token in every document. This helps determine the important words from the unimportant ones.</p>
<div class="highlight"><pre><code class="py language-py" data-lang="py"><span class="n">idf_values</span> <span class="o">=</span> <span class="n">inverse_document_frequencies</span><span class="p">(</span><span class="n">tokenized_documents</span><span class="p">)</span>
<span class="k">print</span> <span class="n">idf_values</span><span class="p">[</span><span class="s">&#39;abenomics&#39;</span><span class="p">]</span>
<span class="c"># 2.94591014906</span>
<span class="k">print</span> <span class="n">idf_values</span><span class="p">[</span><span class="s">&#39;the&#39;</span><span class="p">]</span>
<span class="c"># 1.33647223662</span>
</code></pre></div>
<p>Now we see the power of TF-IDF. We&#39;ve taken the inverse of a token&#39;s document frequency. This means that words that appear a lot like &quot;the&quot; are weighted much less than words like &quot;abenomics&quot;. However TF-IDF allows us to do so much more. We can create vector representations of sentences or more simply, we can transform documents into numbers and lists of documents into matrices.</p>

<p>While this seems relatively unimportant, it actually allows us to do a lot of exception things. We have a mathematical representation of a sentence and we can now use that representation in mathematical ways.</p>
<div class="highlight"><pre><code class="py language-py" data-lang="py"><span class="k">def</span> <span class="nf">tfidf</span><span class="p">(</span><span class="n">documents</span><span class="p">):</span>
    <span class="n">tokenized_documents</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenize</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">]</span>
    <span class="n">idf</span> <span class="o">=</span> <span class="n">inverse_document_frequencies</span><span class="p">(</span><span class="n">tokenized_documents</span><span class="p">)</span>
    <span class="n">tfidf_documents</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">tokenized_documents</span><span class="p">:</span>
        <span class="n">doc_tfidf</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="n">idf</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">tf</span> <span class="o">=</span> <span class="n">sublinear_term_frequency</span><span class="p">(</span><span class="n">term</span><span class="p">,</span> <span class="n">document</span><span class="p">)</span>
            <span class="n">doc_tfidf</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span> <span class="o">*</span> <span class="n">idf</span><span class="p">[</span><span class="n">term</span><span class="p">])</span>
        <span class="n">tfidf_documents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">doc_tfidf</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tfidf_documents</span>
</code></pre></div>
<p>Now that we&#39;ve got a way of performing tfidf, let&#39;s run it!</p>
<div class="highlight"><pre><code class="py language-py" data-lang="py"><span class="n">tfidf_representation</span> <span class="o">=</span> <span class="n">tfidf</span><span class="p">(</span><span class="n">all_documents</span><span class="p">)</span>
<span class="k">print</span> <span class="n">tfidf_representation</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">document_0</span>
<span class="c"># doc vector and document</span>
</code></pre></div>
<p>These two representations are functionally equivalent at this point. Now up until this point we&#39;ve done all this by hand, while it&#39;s been a good exercise there are packages that implement this much more quickly - like Scikit-Learn.</p>

<p>Let&#39;s take a look at how we might code this in Scikit-Learn.</p>
<div class="highlight"><pre><code class="py language-py" data-lang="py"><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>

<span class="n">sklearn_tfidf</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">norm</span><span class="o">=</span><span class="s">&#39;l2&#39;</span><span class="p">,</span><span class="n">min_df</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">use_idf</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">smooth_idf</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">sublinear_tf</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenize</span><span class="p">)</span>

<span class="n">sklearn_representation</span> <span class="o">=</span> <span class="n">sklearn_tfidf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">all_documents</span><span class="p">)</span>
<span class="k">print</span> <span class="n">tfidf_representation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span> <span class="n">sklearn_representation</span><span class="o">.</span><span class="n">toarray</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="k">print</span> <span class="n">document_0</span>
</code></pre></div>
<p>While you will notice that the values in each matrix are not the same because Scikit-learn presents the IDF ordering in a different way (in that fitted model). However we can see in the next part of this tutorial that the exact numbers are irrelevant, it is the vectors that are important. <a href="/tutorials/2014/12/22/cosine-similarity-explained-in-python.html">The next post focuses on cosine similarity or the Euclidean dot product formula in python.</a></p>

<p>Here&#39;s all of our code so far:
<script src="https://gist.github.com/anabranch/02217b94ef02ea13c7ee.js"> </script></p>

<p>Further References:</p>

<ul>
<li><a href="http://en.wikipedia.org/wiki/Tf%E2%80%93idf">TF-IDF</a></li>
<li><a href="http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text">Scikit-Learn TF-IDF</a></li>
</ul>

      </article>
   </div>
</div>

              <div class="footer">
   <!-- <h1>The Portfolio & Writings of Bill Chambers</h1> -->
   <!-- <h3>Internal Navigation</h3> -->
   <div class="row">
      <div class="one offset-by-two columns nav-blocks">
         <a onmouseover="vis_on('fhm');" onmouseout="vis_off('fhm');" href="/"><i class="mdi mdi-home"></i></a>
      </div>
      <div class="one columns nav-blocks">
         <a onmouseover="vis_on('fab');" onmouseout="vis_off('fab');" href="/about.html"><i class="mdi mdi-account"></i></a>
      </div>
      <div class="one columns nav-blocks">
         <a onmouseover="vis_on('fes');" onmouseout="vis_off('fes');" href="/essays.html"><i class="mdi mdi-file-document"></i></a>
      </div>
      <div class="one columns nav-blocks">
         <a onmouseover="vis_on('fpr');" onmouseout="vis_off('fpr');" href="/projects.html"><i class="mdi mdi-beaker"></i></a>
      </div>
      <div class="one columns nav-blocks">
         <a onmouseover="vis_on('ftu');" onmouseout="vis_off('ftu');" href="/tutorials.html"><i class="mdi mdi-cog"></i></a>
      </div>
      <div class="one columns nav-blocks">
         <a onmouseover="vis_on('fli');" onmouseout="vis_off('fli');" href="https://www.linkedin.com/in/wachambers"><i class="mdi mdi-linkedin"></i></a>
      </div>
      <div class="one columns nav-blocks">
         <a onmouseover="vis_on('fgh');" onmouseout="vis_off('fgh');" href="http://github.com/anabranch/"><i class="mdi mdi-github-box"></i></a>
      </div>
      <div class="one columns nav-blocks">
         <a onmouseover="vis_on('ftw');" onmouseout="vis_off('ftw');" href="http://twitter.com/b_a_chambers/"><i class="mdi mdi-twitter-box"></i></a>
      </div>
   </div>
   <div class="row">
      <div class="one offset-by-two columns nav-blocks">
         <a onmouseover="vis_on('fhm');" onmouseout="vis_off('fhm');" id='fhm' class='sibling' href="/">home</a>
      </div>
      <div class="one columns nav-blocks">
         <a onmouseover="vis_on('fab');" onmouseout="vis_off('fab');" id='fab' class='sibling' href="/about.html">about</a>
      </div>
      <div class="one columns nav-blocks">
         <a onmouseover="vis_on('fes');" onmouseout="vis_off('fes');" id='fes' class='sibling' href="/essays.html">essays</a>
      </div>
      <div class="one columns nav-blocks">
         <a onmouseover="vis_on('fpr');" onmouseout="vis_off('fpr');" id='fpr' class='sibling' href="/projects.html">projects</a>
      </div>
      <div class="one columns nav-blocks">
         <a onmouseover="vis_on('ftu');" onmouseout="vis_off('ftu');" id='ftu' class='sibling' href="/tutorials.html">tutorials</a>
      </div>
      <div class="one columns nav-blocks">
         <a onmouseover="vis_on('fli');" onmouseout="vis_off('fli');" id='fli' class='sibling' href="https://www.linkedin.com/in/wachambers">linkedin</a>
      </div>
      <div class="one columns nav-blocks">
         <a onmouseover="vis_on('fgh');" onmouseout="vis_off('fgh');" id='fgh' class='sibling' href="http://github.com/anabranch/">github</a>
      </div>
      <div class="one columns nav-blocks">
         <a onmouseover="vis_on('ftw');" onmouseout="vis_off('ftw');" id='ftw' class='sibling' href="http://twitter.com/b_a_chambers/">twitter</a>
      </div>
   </div>
</div>

        </div> <!-- /container -->
    </body>
    
<script charset="utf-8">
var hovered_el;
function vis_on(id) {
    var e = document.getElementById(id);
        e.style.display = 'block';
    var siblings = document.getElementsByClassName("sibling");
    hovered_el = e;
    for (var i = 0; i < siblings.length; i++) {
        if (siblings[i].id !== id) {
            siblings[i].style.display = 'none';
        }
    }
}

function vis_off(id) {
    hovered_el = undefined;
    setTimeout(function(){
        if (hovered_el == undefined) {
            var e = document.getElementById(id);
                e.style.display = 'none';
        }
    }, 400);
}

setTimeout(function(){
["hhm","hab","hes","hpr","htu","hem", 'fhm', 'fab', 'fes', 'fpr', 'ftu', 'fli', 'fgh', 'ftw'].map(vis_off)
}, 1000);

</script>

</html>
